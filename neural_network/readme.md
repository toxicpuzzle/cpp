# Neural Network Implementation

Neural network implementation using OOP paradigmn in CPP that allows users 
to use either RELU activation function or a sigmoid activation function (without
biases) for each neuron. Backpropogation is implemented without matrix multiplication,
and it can be demonstrated that mean loss lowers over time with the XOR problem,
e.g. learning rate = 0.03, epoch = 100, activation = RELU.